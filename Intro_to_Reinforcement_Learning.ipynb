{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSStbnJAAiR9H0joLoSGqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinezAddiego/GU-NSCI526-Tutorials/blob/main/Intro_to_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Reinforcement Learning Tutorial\n",
        "\n",
        "Flo Martinez Addiego (fam53@georgetown.edu)\n",
        "\n",
        "NSCI-526 Computational Neuroscience\n",
        "\n",
        "Georgetown University\n",
        "\n",
        "This tutorial has been created with the help of various resources including --\n",
        "\n",
        "\n",
        "*   [Supervised vs Unsupervised Learning](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)\n",
        "*   [Neuromatch: Reinforcement Learning](https://compneuro.neuromatch.io/tutorials/W3D4_ReinforcementLearning/student/W3D4_Intro.html)\n",
        "*   [Reinforcement Learning](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292?gi=d6409e549264)\n",
        "*   [Value Iteration vs Policy Iteration in Reinforcement Learning](https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration#:~:text=In%20policy%20iteration%2C%20we%20start%20with%20a%20fixed%20policy.,iteration%20algorithm%20updates%20the%20policy.)\n",
        "*   [OpenAI: Deep Deterministic Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)\n"
      ],
      "metadata": {
        "id": "rVCCZ_HaOH1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Background\n",
        "\n",
        "In class, we have talked a little bit about two different types of learning:\n",
        "1. Supervised Learning: Has a \"teacher\". The data are well-labeled and tagged with the correct answers. (Think back to the Deep Learning Tutorial where we had images of cats and dogs and the CNN learned to distinguish between the two)\n",
        "2. Unsupervised Learning: Does not have a teacher. The algorithm by itself learns what is important. The model finds different patterns\n",
        "\n",
        "But...we know this isn't how humans *typically* learn...What do I mean by that?"
      ],
      "metadata": {
        "id": "i6AJfqwHxITK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enter our new friends\n",
        "\n",
        "![picture](https://img.freepik.com/premium-vector/baby-group-multiethnic-babies-sitting-isolated-cartoon-cute-toddlers-vector-set-child-girl-boy-unfant-kids-cartoon-happy_80590-12100.jpg?w=2000)\n",
        "\n",
        "Suppose there is a fire burning in a fireplace. The kids find out very quickly that if they get close to the fire, they feel warm!\n",
        "\n",
        "So, let's say Fire + [Distance X] has a valence (a value) of +1\n",
        "\n",
        "Now, they don't really know much about their environment, because they're so little. So one day, they get really close to the fire and touch it just enough to feel a little bit of pain! This would be a negative valence because it hurt, and they have now learned they don't want to repeat that.\n",
        "\n",
        "We could rewrite our earlier idea:\n",
        "\n",
        "if distance > 2:\n",
        "fire + distance = +1\n",
        "else:\n",
        "fire + distance = -1\n",
        "\n",
        "All this to say, interacting with their environments gave a positive or a negative reward."
      ],
      "metadata": {
        "id": "ElgL4pj00Md5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In comes Reinforcement Learning\n",
        "\n",
        "We know that all types of machine learning are trying to minimze prediction errors: **reinforcement learning** is no different!\n",
        "\n",
        "As we talked about in the math tutorial, reinforcement learning helps us make a decision based on our predictions of current and future reward (and the cost too!).\n",
        "\n",
        "Though supervised and unsupervised learning look for a match between input/output pairs, reinforcement learning leverages feedback from the environment given back to the agent after performing an action. This feedback often comes in punishment or reward, signaling negative or positive behaviors respectively. Reinforcement learning wants to find the best policy that helps it maximize total cumulative reward."
      ],
      "metadata": {
        "id": "b3fUXCZi3LKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computational Framework\n",
        "\n",
        "![Reinforcement Learning Framework Picture](data:image/png;base64,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)\n",
        "\n",
        "This figure is from [a medium article on reinforcement learning](https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292?gi=d6409e549264). Let's go ahead and break it down (you can consider yourself to be the agent)\n",
        "\n",
        "*   The **environment** refers to the space in which the agent is operating\n",
        "*   The **state** refers to the agent's current situation\n",
        "*   The **reward** refers to the feedback the agent gets from his/her environment (note: in this case the reward can be positive or negative)\n",
        "*   The **policy** refers to the course of action the agent is taking to respond to their state\n",
        "*   The **value** refers to the reward a given course of action or decision will give the agent\n"
      ],
      "metadata": {
        "id": "RpTb3fSe3cEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reinforcement Learning Frameworks\n",
        "\n",
        "\n",
        "There are a lot of different frameworks that use reinforcement learning. I'll mention a few very briefly below to develop some sort of an intuition.\n",
        "\n",
        "Markov-Decision Processes: This is a mathematical framework in which the current state depends ONLY on the immediate prior action/state. It does not have a \"history log\" of what happened in the past. But, in the MDP, we can make a model of our current environment we have a finite set of states, a set of actions, and a set of rewards.\n",
        "\n",
        "How would we go about solving an MDP? We would take advantage of Dynamic Programming in which we do Value Iteration and Policy Iteration. The ultimate goal of this would be to find an optimal policy.\n",
        "\n",
        "![table](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-e006cee9ed0654d81378c599ddacce15_l3.svg)\n",
        "\n",
        "\n",
        "\n",
        "Ok, but what happens when we are in a complex environment and we cannot easily model our environment? Just like when we had unsupervised learning, there also exists \"Model Free Reinforcement Learning\". Model Free RL learns directly from experience/trial-and-error and uses feedback to update internal policies/value functions [[Comparison between Model-Free vs Model-Based Reinforcement Learning]](https://www.tomorrow.bio/post/comparison-between-model-free-vs-model-based-reinforcement-learning-2023-06-4669575769-ai#:~:text=Model%2Dfree%20reinforcement%20learning%20is,internal%20policies%20or%20value%20functions.).\n",
        "\n",
        "An image to see the difference:\n",
        "![image](https://assets-global.website-files.com/62792c49f89cb381b3affec5/649acaeebb680e4a7ac4e154_o0SP05CZTDEM7Hn3qais336jpq4pAi_IaTzMvT9NDNdIx3tJd53hx-Tng8QWr9BPMkeen_H84G1a--E2Fqq9D1ArG4djyFhE61FP9xFucCoU-VMDFlhzGmzxqQ54Ejs4QvzdM39plrVHJHNbCmIl2l4.png)\n",
        "\n",
        "\n",
        "Some Model-Free Methods:\n",
        "1. The Monte Carlo method. Here the agent interacts with the environment to learn about different states/rewards. Basically, in a Monte Carlo algoirthm, we are repeatedly sampling the environment to predict possible outcomes of an uncertain event.\n",
        "2. Temporal Difference Learning method. We look at our current action and the future actions to estimate current and future award so that we have the best policy. (You guys should be experts on this by now)\n",
        "\n",
        "\n",
        "Building the optimal policy...Exploration vs Exploitation.\n",
        "\n",
        "If we do not have a complete model of our environment (more realistic), then we need to both create a model based on our current understanding as well as continue to explore other options so that we can eventually learn the ultimate best policy.\n",
        "\n",
        "Let's talk about this picture to get a better understanding.\n",
        "![image](https://previews.123rf.com/images/svnikitin/svnikitin1911/svnikitin191100041/137247490-cute-mouse-s-maze-game-help-mouse-to-find-his-cheese-maze-puzzle-with-solution.jpg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CrE6LObFAiHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model-Free Algorithms:\n",
        "\n",
        "1. Q-Learning: Is a model-free approach that allows for the model to iteratively learn and improve over time by taking actions. The agent gets to make predictions about the environment by itself and then learns from this. Q-learning is **off-policy** meaning that determines the optimal action solely based on its current state. No defined policy is needed. Mathematically, the model focuses on updating Q values (value of performin an action in a state). [tech target](https://www.techtarget.com/searchenterpriseai/definition/Q-learning#:~:text=Q%2Dlearning%20is%20a%20machine,way%20animals%20or%20children%20learn.).\n",
        "\n",
        "2. Deep Deterministic Policy Gradient Learning: This is another model-free approach. It uses off-policy data to learn the Q-function, and then later uses the Q-function to learn the policy. It takes advantage of an actor-critic algorithm to learn a lot of policies in high-dimesional spaces.\n",
        "\n",
        "Actor-Critic Model\n",
        "\n",
        "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*azzV78wFkRq9ePrzGnvf5Q.png)"
      ],
      "metadata": {
        "id": "Ku5IlP5YKxB6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSQViw9r7Iup"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}